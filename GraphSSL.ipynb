{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GraphSSL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXqz7ZRo5E5O"
      },
      "source": [
        "# Install required packages.\n",
        "!pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
        "!pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
        "!pip install -q torch-geometric\n",
        "\n",
        "# Helper function for visualization.\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "def visualize(h, color):\n",
        "    z = TSNE(n_components=2).fit_transform(out.detach().cpu().numpy())\n",
        "\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "\n",
        "    plt.scatter(z[:, 0], z[:, 1], s=70, c=color, cmap=\"Set2\")\n",
        "    plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHiE9jKB5Oo0"
      },
      "source": [
        "import os\n",
        "import os.path as osp\n",
        "import shutil\n",
        "\n",
        "import torch\n",
        "from torch_geometric.data import InMemoryDataset, download_url, extract_zip\n",
        "from torch_geometric.io import read_tu_data\n",
        "\n",
        "from itertools import repeat, product\n",
        "import numpy as np\n",
        "\n",
        "from copy import deepcopy\n",
        "import pdb\n",
        "\n",
        "\n",
        "class TUDataset_aug(InMemoryDataset):\n",
        "    r\"\"\"A variety of graph kernel benchmark datasets, *.e.g.* \"IMDB-BINARY\",\n",
        "    \"REDDIT-BINARY\" or \"PROTEINS\", collected from the `TU Dortmund University\n",
        "    <https://chrsmrrs.github.io/datasets>`_.\n",
        "    In addition, this dataset wrapper provides `cleaned dataset versions\n",
        "    <https://github.com/nd7141/graph_datasets>`_ as motivated by the\n",
        "    `\"Understanding Isomorphism Bias in Graph Data Sets\"\n",
        "    <https://arxiv.org/abs/1910.12091>`_ paper, containing only non-isomorphic\n",
        "    graphs.\n",
        "\n",
        "    .. note::\n",
        "        Some datasets may not come with any node labels.\n",
        "        You can then either make use of the argument :obj:`use_node_attr`\n",
        "        to load additional continuous node attributes (if present) or provide\n",
        "        synthetic node features using transforms such as\n",
        "        like :class:`torch_geometric.transforms.Constant` or\n",
        "        :class:`torch_geometric.transforms.OneHotDegree`.\n",
        "\n",
        "    Args:\n",
        "        root (string): Root directory where the dataset should be saved.\n",
        "        name (string): The `name\n",
        "            <https://chrsmrrs.github.io/datasets/docs/datasets/>`_ of the\n",
        "            dataset.\n",
        "        transform (callable, optional): A function/transform that takes in an\n",
        "            :obj:`torch_geometric.data.Data` object and returns a transformed\n",
        "            version. The data object will be transformed before every access.\n",
        "            (default: :obj:`None`)\n",
        "        pre_transform (callable, optional): A function/transform that takes in\n",
        "            an :obj:`torch_geometric.data.Data` object and returns a\n",
        "            transformed version. The data object will be transformed before\n",
        "            being saved to disk. (default: :obj:`None`)\n",
        "        pre_filter (callable, optional): A function that takes in an\n",
        "            :obj:`torch_geometric.data.Data` object and returns a boolean\n",
        "            value, indicating whether the data object should be included in the\n",
        "            final dataset. (default: :obj:`None`)\n",
        "        use_node_attr (bool, optional): If :obj:`True`, the dataset will\n",
        "            contain additional continuous node attributes (if present).\n",
        "            (default: :obj:`False`)\n",
        "        use_edge_attr (bool, optional): If :obj:`True`, the dataset will\n",
        "            contain additional continuous edge attributes (if present).\n",
        "            (default: :obj:`False`)\n",
        "        cleaned: (bool, optional): If :obj:`True`, the dataset will\n",
        "            contain only non-isomorphic graphs. (default: :obj:`False`)\n",
        "    \"\"\"\n",
        "\n",
        "    url = 'https://www.chrsmrrs.com/graphkerneldatasets'\n",
        "    cleaned_url = ('https://raw.githubusercontent.com/nd7141/'\n",
        "                   'graph_datasets/master/datasets')\n",
        "\n",
        "    def __init__(self, root, name, transform=None, pre_transform=None,\n",
        "                 pre_filter=None, use_node_attr=False, use_edge_attr=False,\n",
        "                 cleaned=False, aug=None):\n",
        "        self.name = name\n",
        "        self.cleaned = cleaned\n",
        "        super(TUDataset_aug, self).__init__(root, transform, pre_transform,\n",
        "                                        pre_filter)\n",
        "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
        "        if self.data.x is not None and not use_node_attr:\n",
        "            num_node_attributes = self.num_node_attributes\n",
        "            self.data.x = self.data.x[:, num_node_attributes:]\n",
        "        if self.data.edge_attr is not None and not use_edge_attr:\n",
        "            num_edge_attributes = self.num_edge_attributes\n",
        "            self.data.edge_attr = self.data.edge_attr[:, num_edge_attributes:]\n",
        "        if not (self.name == 'MUTAG' or self.name == 'PTC_MR' or self.name == 'DD' or self.name == 'PROTEINS' or self.name == 'NCI1' or self.name == 'NCI109'):\n",
        "            edge_index = self.data.edge_index[0, :].numpy()\n",
        "            _, num_edge = self.data.edge_index.size()\n",
        "            nlist = [edge_index[n] + 1 for n in range(num_edge - 1) if edge_index[n] > edge_index[n + 1]]\n",
        "            nlist.append(edge_index[-1] + 1)\n",
        "\n",
        "            num_node = np.array(nlist).sum()\n",
        "            self.data.x = torch.ones((num_node, 1))\n",
        "\n",
        "            edge_slice = [0]\n",
        "            k = 0\n",
        "            for n in nlist:\n",
        "                k = k + n\n",
        "                edge_slice.append(k)\n",
        "            self.slices['x'] = torch.tensor(edge_slice)\n",
        "\n",
        "            '''\n",
        "            print(self.data.x.size())\n",
        "            print(self.slices['x'])\n",
        "            print(self.slices['x'].size())\n",
        "            assert False\n",
        "            '''\n",
        "\n",
        "        self.aug = aug\n",
        "\n",
        "    @property\n",
        "    def raw_dir(self):\n",
        "        name = 'raw{}'.format('_cleaned' if self.cleaned else '')\n",
        "        return osp.join(self.root, self.name, name)\n",
        "\n",
        "    @property\n",
        "    def processed_dir(self):\n",
        "        name = 'processed{}'.format('_cleaned' if self.cleaned else '')\n",
        "        return osp.join(self.root, self.name, name)\n",
        "\n",
        "    @property\n",
        "    def num_node_labels(self):\n",
        "        if self.data.x is None:\n",
        "            return 0\n",
        "        for i in range(self.data.x.size(1)):\n",
        "            x = self.data.x[:, i:]\n",
        "            if ((x == 0) | (x == 1)).all() and (x.sum(dim=1) == 1).all():\n",
        "                return self.data.x.size(1) - i\n",
        "        return 0\n",
        "\n",
        "    @property\n",
        "    def num_node_attributes(self):\n",
        "        if self.data.x is None:\n",
        "            return 0\n",
        "        return self.data.x.size(1) - self.num_node_labels\n",
        "\n",
        "    @property\n",
        "    def num_edge_labels(self):\n",
        "        if self.data.edge_attr is None:\n",
        "            return 0\n",
        "        for i in range(self.data.edge_attr.size(1)):\n",
        "            if self.data.edge_attr[:, i:].sum() == self.data.edge_attr.size(0):\n",
        "                return self.data.edge_attr.size(1) - i\n",
        "        return 0\n",
        "\n",
        "    @property\n",
        "    def num_edge_attributes(self):\n",
        "        if self.data.edge_attr is None:\n",
        "            return 0\n",
        "        return self.data.edge_attr.size(1) - self.num_edge_labels\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        names = ['A', 'graph_indicator']\n",
        "        return ['{}_{}.txt'.format(self.name, name) for name in names]\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        return 'data.pt'\n",
        "\n",
        "    def download(self):\n",
        "        url = self.cleaned_url if self.cleaned else self.url\n",
        "        folder = osp.join(self.root, self.name)\n",
        "        path = download_url('{}/{}.zip'.format(url, self.name), folder)\n",
        "        extract_zip(path, folder)\n",
        "        os.unlink(path)\n",
        "        shutil.rmtree(self.raw_dir)\n",
        "        os.rename(osp.join(folder, self.name), self.raw_dir)\n",
        "\n",
        "    def process(self):\n",
        "        self.data, self.slices = read_tu_data(self.raw_dir, self.name)\n",
        "\n",
        "        if self.pre_filter is not None:\n",
        "            data_list = [self.get(idx) for idx in range(len(self))]\n",
        "            data_list = [data for data in data_list if self.pre_filter(data)]\n",
        "            self.data, self.slices = self.collate(data_list)\n",
        "\n",
        "        if self.pre_transform is not None:\n",
        "            data_list = [self.get(idx) for idx in range(len(self))]\n",
        "            data_list = [self.pre_transform(data) for data in data_list]\n",
        "            self.data, self.slices = self.collate(data_list)\n",
        "\n",
        "        torch.save((self.data, self.slices), self.processed_paths[0])\n",
        "\n",
        "    def __repr__(self):\n",
        "        return '{}({})'.format(self.name, len(self))\n",
        "\n",
        "    def get_num_feature(self):\n",
        "        data = self.data.__class__()\n",
        "\n",
        "        if hasattr(self.data, '__num_nodes__'):\n",
        "            data.num_nodes = self.data.__num_nodes__[0]\n",
        "\n",
        "        for key in self.data.keys:\n",
        "            item, slices = self.data[key], self.slices[key]\n",
        "            if torch.is_tensor(item):\n",
        "                s = list(repeat(slice(None), item.dim()))\n",
        "                s[self.data.__cat_dim__(key,\n",
        "                                        item)] = slice(slices[0],\n",
        "                                                       slices[0 + 1])\n",
        "            else:\n",
        "                s = slice(slices[idx], slices[idx + 1])\n",
        "            data[key] = item[s]\n",
        "        _, num_feature = data.x.size()\n",
        "\n",
        "        return num_feature\n",
        "\n",
        "\n",
        "    def get(self, idx):\n",
        "        data = self.data.__class__()\n",
        "\n",
        "        if hasattr(self.data, '__num_nodes__'):\n",
        "            data.num_nodes = self.data.__num_nodes__[idx]\n",
        "\n",
        "        for key in self.data.keys:\n",
        "            item, slices = self.data[key], self.slices[key]\n",
        "            if torch.is_tensor(item):\n",
        "                s = list(repeat(slice(None), item.dim()))\n",
        "                s[self.data.__cat_dim__(key,\n",
        "                                        item)] = slice(slices[idx],\n",
        "                                                       slices[idx + 1])\n",
        "            else:\n",
        "                s = slice(slices[idx], slices[idx + 1])\n",
        "            data[key] = item[s]\n",
        "\n",
        "        \"\"\"\n",
        "        edge_index = data.edge_index\n",
        "        node_num = data.x.size()[0]\n",
        "        edge_num = data.edge_index.size()[1]\n",
        "        data.edge_index = torch.tensor([[edge_index[0, n], edge_index[1, n]] for n in range(edge_num) if edge_index[0, n] < node_num and edge_index[1, n] < node_num] + [[n, n] for n in range(node_num)], dtype=torch.int64).t()\n",
        "        \"\"\"\n",
        "\n",
        "        node_num = data.edge_index.max()\n",
        "        sl = torch.tensor([[n,n] for n in range(node_num)]).t()\n",
        "        data.edge_index = torch.cat((data.edge_index, sl), dim=1)\n",
        "\n",
        "        if self.aug == 'dnodes':\n",
        "            data_aug = drop_nodes(deepcopy(data), drop_factor = 10)\n",
        "        elif self.aug == 'pedges':\n",
        "            data_aug = permute_edges(deepcopy(data))\n",
        "        elif self.aug == 'subgraph':\n",
        "            data_aug = subgraph(deepcopy(data))\n",
        "        elif self.aug == 'mask_nodes':\n",
        "            data_aug = mask_nodes(deepcopy(data))\n",
        "        elif self.aug == 'none':\n",
        "            \"\"\"\n",
        "            if data.edge_index.max() > data.x.size()[0]:\n",
        "                print(data.edge_index)\n",
        "                print(data.x.size())\n",
        "                assert False\n",
        "            \"\"\"\n",
        "            data_aug = deepcopy(data)\n",
        "            data_aug.x = torch.ones((data.edge_index.max()+1, 1))\n",
        "\n",
        "        elif self.aug == 'random2':\n",
        "            n = np.random.randint(2)\n",
        "            if n == 0:\n",
        "               data_aug = drop_nodes(deepcopy(data))\n",
        "            elif n == 1:\n",
        "               data_aug = subgraph(deepcopy(data))\n",
        "            else:\n",
        "                print('sample error')\n",
        "                assert False\n",
        "\n",
        "\n",
        "        elif self.aug == 'random3':\n",
        "            n = np.random.randint(3)\n",
        "            if n == 0:\n",
        "               data_aug = drop_nodes(deepcopy(data))\n",
        "            elif n == 1:\n",
        "               data_aug = permute_edges(deepcopy(data))\n",
        "            elif n == 2:\n",
        "               data_aug = subgraph(deepcopy(data))\n",
        "            else:\n",
        "                print('sample error')\n",
        "                assert False\n",
        "\n",
        "\n",
        "        elif self.aug == 'random4':\n",
        "            n = np.random.randint(4)\n",
        "            if n == 0:\n",
        "               data_aug = drop_nodes(deepcopy(data))\n",
        "            elif n == 1:\n",
        "               data_aug = permute_edges(deepcopy(data))\n",
        "            elif n == 2:\n",
        "               data_aug = subgraph(deepcopy(data))\n",
        "            elif n == 3:\n",
        "               data_aug = mask_nodes(deepcopy(data))\n",
        "            else:\n",
        "                print('sample error')\n",
        "                assert False\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        else:\n",
        "            print('augmentation error')\n",
        "            assert False\n",
        "\n",
        "        # print(data, data_aug)\n",
        "        # assert False\n",
        "\n",
        "        return data, data_aug\n",
        "\n",
        "\n",
        "def drop_nodes(data, drop_factor = 10):\n",
        "\n",
        "    node_num, _ = data.x.size()\n",
        "    _, edge_num = data.edge_index.size()\n",
        "    drop_num = int(node_num / drop_factor)\n",
        "\n",
        "    idx_drop = np.random.choice(node_num, drop_num, replace=False)\n",
        "    idx_nondrop = [n for n in range(node_num) if not n in idx_drop]\n",
        "    idx_dict = {idx_nondrop[n]:n for n in list(range(node_num - drop_num))}\n",
        "\n",
        "    # data.x = data.x[idx_nondrop]\n",
        "    edge_index = data.edge_index.numpy()\n",
        "\n",
        "    adj = torch.zeros((node_num, node_num))\n",
        "    adj[edge_index[0], edge_index[1]] = 1\n",
        "    adj[idx_drop, :] = 0\n",
        "    adj[:, idx_drop] = 0\n",
        "    edge_index = adj.nonzero().t()\n",
        "\n",
        "    data.edge_index = edge_index\n",
        "\n",
        "    # edge_index = [[idx_dict[edge_index[0, n]], idx_dict[edge_index[1, n]]] for n in range(edge_num) if (not edge_index[0, n] in idx_drop) and (not edge_index[1, n] in idx_drop)]\n",
        "    # edge_index = [[edge_index[0, n], edge_index[1, n]] for n in range(edge_num) if (not edge_index[0, n] in idx_drop) and (not edge_index[1, n] in idx_drop)] + [[n, n] for n in idx_nondrop]\n",
        "    # data.edge_index = torch.tensor(edge_index).transpose_(0, 1)\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def permute_edges(data, perturb_factor = 10):\n",
        "\n",
        "    node_num, _ = data.x.size()\n",
        "    _, edge_num = data.edge_index.size()\n",
        "    permute_num = int(edge_num / perturb_factor)\n",
        "\n",
        "    edge_index = data.edge_index.transpose(0, 1).numpy()\n",
        "\n",
        "    idx_add = np.random.choice(node_num, (permute_num, 2))\n",
        "    # idx_add = [[idx_add[0, n], idx_add[1, n]] for n in range(permute_num) if not (idx_add[0, n], idx_add[1, n]) in edge_index]\n",
        "\n",
        "    # edge_index = np.concatenate((np.array([edge_index[n] for n in range(edge_num) if not n in np.random.choice(edge_num, permute_num, replace=False)]), idx_add), axis=0)\n",
        "    # edge_index = np.concatenate((edge_index[np.random.choice(edge_num, edge_num-permute_num, replace=False)], idx_add), axis=0)\n",
        "    edge_index = edge_index[np.random.choice(edge_num, edge_num-permute_num, replace=False)]\n",
        "    # edge_index = [edge_index[n] for n in range(edge_num) if not n in np.random.choice(edge_num, permute_num, replace=False)] + idx_add\n",
        "    data.edge_index = torch.tensor(edge_index).transpose_(0, 1)\n",
        "\n",
        "    return data\n",
        "\n",
        "def subgraph(data, graph_factor = 0.2):\n",
        "\n",
        "    node_num, _ = data.x.size()\n",
        "    _, edge_num = data.edge_index.size()\n",
        "    sub_num = int(node_num * graph_factor)\n",
        "\n",
        "    edge_index = data.edge_index.numpy()\n",
        "\n",
        "    idx_sub = [np.random.randint(node_num, size=1)[0]]\n",
        "    idx_neigh = set([n for n in edge_index[1][edge_index[0]==idx_sub[0]]])\n",
        "\n",
        "    count = 0\n",
        "    while len(idx_sub) <= sub_num:\n",
        "        count = count + 1\n",
        "        if count > node_num:\n",
        "            break\n",
        "        if len(idx_neigh) == 0:\n",
        "            break\n",
        "        sample_node = np.random.choice(list(idx_neigh))\n",
        "        if sample_node in idx_sub:\n",
        "            continue\n",
        "        idx_sub.append(sample_node)\n",
        "        idx_neigh.union(set([n for n in edge_index[1][edge_index[0]==idx_sub[-1]]]))\n",
        "\n",
        "    idx_drop = [n for n in range(node_num) if not n in idx_sub]\n",
        "    idx_nondrop = idx_sub\n",
        "    idx_dict = {idx_nondrop[n]:n for n in list(range(len(idx_nondrop)))}\n",
        "\n",
        "    # data.x = data.x[idx_nondrop]\n",
        "    edge_index = data.edge_index.numpy()\n",
        "\n",
        "    adj = torch.zeros((node_num, node_num))\n",
        "    adj[edge_index[0], edge_index[1]] = 1\n",
        "    adj[idx_drop, :] = 0\n",
        "    adj[:, idx_drop] = 0\n",
        "    edge_index = adj.nonzero().t()\n",
        "\n",
        "    data.edge_index = edge_index\n",
        "\n",
        "\n",
        "\n",
        "    # edge_index = [[idx_dict[edge_index[0, n]], idx_dict[edge_index[1, n]]] for n in range(edge_num) if (not edge_index[0, n] in idx_drop) and (not edge_index[1, n] in idx_drop)]\n",
        "    # edge_index = [[edge_index[0, n], edge_index[1, n]] for n in range(edge_num) if (not edge_index[0, n] in idx_drop) and (not edge_index[1, n] in idx_drop)] + [[n, n] for n in idx_nondrop]\n",
        "    # data.edge_index = torch.tensor(edge_index).transpose_(0, 1)\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def mask_nodes(data, mask_factor = 10):\n",
        "\n",
        "    node_num, feat_dim = data.x.size()\n",
        "    mask_num = int(node_num / mask_factor)\n",
        "\n",
        "    idx_mask = np.random.choice(node_num, mask_num, replace=False)\n",
        "    data.x[idx_mask] = torch.tensor(np.random.normal(loc=0.5, scale=0.5, size=(mask_num, feat_dim)), dtype=torch.float32)\n",
        "\n",
        "    return data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27TseJfB5SaK"
      },
      "source": [
        "import os.path as osp\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import json\n",
        "# from core.encoders import *\n",
        "\n",
        "# from torch_geometric.datasets import TUDataset\n",
        "# from aug import TUDataset_aug as TUDataset\n",
        "from torch_geometric.data import DataLoader\n",
        "import sys\n",
        "import json\n",
        "from torch import optim\n",
        "\n",
        "from cortex_DIM.nn_modules.mi_networks import MIFCNet, MI1x1ConvNet\n",
        "from losses import *\n",
        "from gin import Encoder\n",
        "from evaluate_embedding import evaluate_embedding\n",
        "from model import *\n",
        "\n",
        "from arguments import arg_parse\n",
        "from torch_geometric.transforms import Constant\n",
        "import pdb\n",
        "\n",
        "\n",
        "class GcnInfomax(nn.Module):\n",
        "  def __init__(self, hidden_dim, num_gc_layers, alpha=0.5, beta=1., gamma=.1):\n",
        "    super(GcnInfomax, self).__init__()\n",
        "\n",
        "    self.alpha = alpha\n",
        "    self.beta = beta\n",
        "    self.gamma = gamma\n",
        "    self.prior = args.prior\n",
        "\n",
        "    self.embedding_dim = mi_units = hidden_dim * num_gc_layers\n",
        "    self.encoder = Encoder(dataset_num_features, hidden_dim, num_gc_layers)\n",
        "\n",
        "    self.local_d = FF(self.embedding_dim)\n",
        "    self.global_d = FF(self.embedding_dim)\n",
        "    # self.local_d = MI1x1ConvNet(self.embedding_dim, mi_units)\n",
        "    # self.global_d = MIFCNet(self.embedding_dim, mi_units)\n",
        "\n",
        "    if self.prior:\n",
        "        self.prior_d = PriorDiscriminator(self.embedding_dim)\n",
        "\n",
        "    self.init_emb()\n",
        "\n",
        "  def init_emb(self):\n",
        "    initrange = -1.5 / self.embedding_dim\n",
        "    for m in self.modules():\n",
        "        if isinstance(m, nn.Linear):\n",
        "            torch.nn.init.xavier_uniform_(m.weight.data)\n",
        "            if m.bias is not None:\n",
        "                m.bias.data.fill_(0.0)\n",
        "\n",
        "\n",
        "  def forward(self, x, edge_index, batch, num_graphs):\n",
        "\n",
        "    # batch_size = data.num_graphs\n",
        "    if x is None:\n",
        "        x = torch.ones(batch.shape[0]).to(device)\n",
        "\n",
        "    y, M = self.encoder(x, edge_index, batch)\n",
        "    \n",
        "    g_enc = self.global_d(y)\n",
        "    l_enc = self.local_d(M)\n",
        "\n",
        "    mode='fd'\n",
        "    measure='JSD'\n",
        "    local_global_loss = local_global_loss_(l_enc, g_enc, edge_index, batch, measure)\n",
        " \n",
        "    if self.prior:\n",
        "        prior = torch.rand_like(y)\n",
        "        term_a = torch.log(self.prior_d(prior)).mean()\n",
        "        term_b = torch.log(1.0 - self.prior_d(y)).mean()\n",
        "        PRIOR = - (term_a + term_b) * self.gamma\n",
        "    else:\n",
        "        PRIOR = 0\n",
        "    \n",
        "    return local_global_loss + PRIOR\n",
        "\n",
        "def off_diagonal(x):\n",
        "    # return a flattened view of the off-diagonal elements of a square matrix\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "class simclr(nn.Module):\n",
        "  def __init__(self, hidden_dim, num_gc_layers, alpha=0.5, beta=1., gamma=.1):\n",
        "    super(simclr, self).__init__()\n",
        "\n",
        "    self.alpha = alpha\n",
        "    self.beta = beta\n",
        "    self.gamma = gamma\n",
        "    self.prior = args.prior\n",
        "\n",
        "    self.embedding_dim = mi_units = hidden_dim * num_gc_layers\n",
        "    self.encoder = Encoder(dataset_num_features, hidden_dim, num_gc_layers)\n",
        "\n",
        "    self.proj_head = nn.Sequential(nn.Linear(self.embedding_dim, self.embedding_dim), nn.ReLU(inplace=True), nn.Linear(self.embedding_dim, self.embedding_dim))\n",
        "\n",
        "    self.init_emb()\n",
        "\n",
        "  def init_emb(self):\n",
        "    initrange = -1.5 / self.embedding_dim\n",
        "    for m in self.modules():\n",
        "        if isinstance(m, nn.Linear):\n",
        "            torch.nn.init.xavier_uniform_(m.weight.data)\n",
        "            if m.bias is not None:\n",
        "                m.bias.data.fill_(0.0)\n",
        "\n",
        "\n",
        "  def forward(self, x, edge_index, batch, num_graphs):\n",
        "\n",
        "    # batch_size = data.num_graphs\n",
        "    if x is None:\n",
        "        x = torch.ones(batch.shape[0]).to(device)\n",
        "\n",
        "    y, M = self.encoder(x, edge_index, batch)\n",
        "    \n",
        "    y = self.proj_head(y)\n",
        "    \n",
        "    return y\n",
        "\n",
        "  def loss_cal(self, x, x_aug):\n",
        "\n",
        "    T = 0.2\n",
        "    batch_size, _ = x.size()\n",
        "    x_abs = x.norm(dim=1)\n",
        "    x_aug_abs = x_aug.norm(dim=1)\n",
        "\n",
        "    sim_matrix = torch.einsum('ik,jk->ij', x, x_aug) / torch.einsum('i,j->ij', x_abs, x_aug_abs)\n",
        "    sim_matrix = torch.exp(sim_matrix / T)\n",
        "    pos_sim = sim_matrix[range(batch_size), range(batch_size)]\n",
        "    loss = pos_sim / (sim_matrix.sum(dim=1) - pos_sim)\n",
        "    loss = - torch.log(loss).mean()\n",
        "\n",
        "    return loss\n",
        "\n",
        "  def BT_loss_cal(self, x, x_aug, corr_neg_one = False, lmbda = 0.005):\n",
        "\n",
        "    batch_size, _ = x.size()\n",
        "    x, x_aug = F.normalize(x, dim=-1), F.normalize(x_aug, dim=-1)\n",
        "    # Barlow Twins\n",
        "    # normalize the representations along the batch dimension\n",
        "    out_1_norm = (x - x.mean(dim=0)) / x.std(dim=0)\n",
        "    out_2_norm = (x_aug - x_aug.mean(dim=0)) / x_aug.std(dim=0)\n",
        "\n",
        "    # cross-correlation matrix\n",
        "    c = torch.matmul(out_1_norm.T, out_2_norm) / batch_size\n",
        "\n",
        "    # loss\n",
        "    on_diag = torch.diagonal(c).add_(-1).pow_(2).sum()\n",
        "    if corr_neg_one is False:\n",
        "        # the loss described in the original Barlow Twin's paper\n",
        "        # encouraging off_diag to be zero\n",
        "        off_diag = off_diagonal(c).pow_(2).sum()\n",
        "    else:\n",
        "        # inspired by HSIC\n",
        "        # encouraging off_diag to be negative ones\n",
        "        off_diag = off_diagonal(c).add_(1).pow_(2).sum()\n",
        "    loss = on_diag + lmbda * off_diag\n",
        "\n",
        "    return loss\n",
        "\n",
        "  def VICReg_loss_cal(self, z_a, z_b, corr_neg_one = False, lmbda = 25, mu = 25, nu = 1):\n",
        "\n",
        "    batch_size, D = z_a.size()\n",
        "    N = batch_size\n",
        "    z_a, z_b = F.normalize(x, dim=-1), F.normalize(x_aug, dim=-1)\n",
        "\n",
        "    # invariance loss\n",
        "    sim_loss = torch.nn.MSELoss()(z_a, z_b)\n",
        "\n",
        "    # variance loss\n",
        "    std_z_a = torch.sqrt(z_a.var(dim=0) + 1e-04)\n",
        "    std_z_b = torch.sqrt(z_b.var(dim=0) + 1e-04)\n",
        "    std_loss = torch.mean(torch.max(torch.zeros_like(torch.tensor(std_z_a.shape[0])).cuda(),(1 - std_z_a)))\n",
        "    std_loss = std_loss + torch.mean(torch.max(torch.zeros_like(torch.tensor(std_z_b.shape[0])).cuda(), (1 - std_z_b)))\n",
        "\n",
        "    # covariance loss\n",
        "    z_a = z_a - z_a.mean(dim=0)\n",
        "    z_b = z_b - z_b.mean(dim=0)\n",
        "    cov_z_a = (z_a.T @ z_a) / (N - 1)\n",
        "    cov_z_b = (z_b.T @ z_b) / (N - 1)\n",
        "\n",
        "    if corr_neg_one is False:\n",
        "        # the loss described in the original Barlow Twin's paper\n",
        "        # encouraging off_diag to be zero\n",
        "        cov_loss = off_diagonal(cov_z_a).pow_(2).sum() / D\n",
        "        cov_loss = cov_loss + off_diagonal(cov_z_b).pow_(2).sum() / D\n",
        "    else:\n",
        "        # inspired by HSIC\n",
        "        # encouraging off_diag to be negative ones\n",
        "        cov_loss = off_diagonal(cov_z_a).add_(1).pow_(2).sum() / D\n",
        "        cov_loss = cov_loss + off_diagonal(cov_z_b).add_(1).pow_(2).sum() / D\n",
        "\n",
        "    # loss\n",
        "    loss = lmbda * sim_loss + mu * std_loss + nu * cov_loss\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "import random\n",
        "def setup_seed(seed):\n",
        "\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # from Bio import SeqIO\n",
        "    # path = '/path/to/sequences.txt'\n",
        "    # sequences = [str(record.seq) for record in  SeqIO.parse(path, 'fasta')]\n",
        "    sys.argv = ['foo'] #['-f'] + sequences\n",
        "    \n",
        "    args = arg_parse()\n",
        "    setup_seed(args.seed)\n",
        "\n",
        "    accuracies = {'val':[], 'test':[]}\n",
        "    epochs = 500\n",
        "    log_interval = 10\n",
        "    batch_size = 128\n",
        "    # batch_size = 512\n",
        "    args.lr = 1e-2\n",
        "    args.DS = 'PROTEINS'\n",
        "    args.aug = 'random2'\n",
        "    lr = args.lr\n",
        "    DS = args.DS\n",
        "    args.hidden_dim = 32 # default = 32  32*2\n",
        "    # args.num_gc_layers = 5 # default = 5\n",
        "    __file__ = 'dataset1'\n",
        "    path = osp.join(osp.dirname(osp.realpath(__file__)), '.', 'data', DS)\n",
        "    # kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=None)\n",
        "\n",
        "    dataset = TUDataset_aug(path, name=DS, aug=args.aug).shuffle()\n",
        "    dataset_eval = TUDataset_aug(path, name=DS, aug='none').shuffle()\n",
        "    print(len(dataset))\n",
        "    print(dataset.get_num_feature())\n",
        "    try:\n",
        "        dataset_num_features = dataset.get_num_feature()\n",
        "    except:\n",
        "        dataset_num_features = 1\n",
        "\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
        "    dataloader_eval = DataLoader(dataset_eval, batch_size=batch_size)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = simclr(args.hidden_dim, args.num_gc_layers).to(device)\n",
        "    # print(model)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, steps_per_epoch=len(dataloader), epochs=epochs)\n",
        "\n",
        "\n",
        "    print('================')\n",
        "    print('lr: {}'.format(lr))\n",
        "    print('num_features: {}'.format(dataset_num_features))\n",
        "    print('hidden_dim: {}'.format(args.hidden_dim))\n",
        "    print('num_gc_layers: {}'.format(args.num_gc_layers))\n",
        "    print('================')\n",
        "\n",
        "    model.eval()\n",
        "    emb, y = model.encoder.get_embeddings(dataloader_eval)\n",
        "    # print(emb.shape, y.shape)\n",
        "\n",
        "    \"\"\"\n",
        "    acc_val, acc = evaluate_embedding(emb, y)\n",
        "    accuracies['val'].append(acc_val)\n",
        "    accuracies['test'].append(acc)\n",
        "    \"\"\"\n",
        "    loss_list = []\n",
        "    for epoch in range(1, epochs+1):\n",
        "        loss_all = 0\n",
        "        model.train()\n",
        "        for data in dataloader:\n",
        "\n",
        "            # print('start')\n",
        "            data, data_aug = data\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            \n",
        "            node_num, _ = data.x.size()\n",
        "            data = data.to(device)\n",
        "            x = model(data.x, data.edge_index, data.batch, data.num_graphs)\n",
        "\n",
        "            if args.aug == 'dnodes' or args.aug == 'subgraph' or args.aug == 'random2' or args.aug == 'random3' or args.aug == 'random4':\n",
        "                # node_num_aug, _ = data_aug.x.size()\n",
        "                edge_idx = data_aug.edge_index.numpy()\n",
        "                _, edge_num = edge_idx.shape\n",
        "                idx_not_missing = [n for n in range(node_num) if (n in edge_idx[0] or n in edge_idx[1])]\n",
        "\n",
        "                node_num_aug = len(idx_not_missing)\n",
        "                data_aug.x = data_aug.x[idx_not_missing]\n",
        "\n",
        "                \n",
        "\n",
        "                data_aug.batch = data.batch[idx_not_missing]\n",
        "                idx_dict = {idx_not_missing[n]:n for n in range(node_num_aug)}\n",
        "                edge_idx = [[idx_dict[edge_idx[0, n]], idx_dict[edge_idx[1, n]]] for n in range(edge_num) if not edge_idx[0, n] == edge_idx[1, n]]\n",
        "                data_aug.edge_index = torch.tensor(edge_idx).transpose_(0, 1)\n",
        "\n",
        "            data_aug = data_aug.to(device)\n",
        "\n",
        "            '''\n",
        "            print(data.edge_index)\n",
        "            print(data.edge_index.size())\n",
        "            print(data_aug.edge_index)\n",
        "            print(data_aug.edge_index.size())\n",
        "            print(data.x.size())\n",
        "            print(data_aug.x.size())\n",
        "            print(data.batch.size())\n",
        "            print(data_aug.batch.size())\n",
        "            pdb.set_trace()\n",
        "            '''\n",
        "\n",
        "            x_aug = model(data_aug.x, data_aug.edge_index, data_aug.batch, data_aug.num_graphs)\n",
        "            \n",
        "            # loss = model.loss_cal(x, x_aug) # SIMCLR\n",
        "            # loss = model.BT_loss_cal(x, x_aug, corr_neg_one = False) # BT\n",
        "            # loss = model.BT_loss_cal(x, x_aug, corr_neg_one = True) # HSIC\n",
        "            # loss = model.VICReg_loss_cal(x, x_aug, corr_neg_one = False) # VICreg\n",
        "            loss = model.VICReg_loss_cal(x, x_aug, corr_neg_one = True) # VICregHSIC\n",
        "            # print(loss)\n",
        "            loss_all += loss.item() * data.num_graphs\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            # print('batch')\n",
        "        loss_list.append(loss_all/ (data.num_graphs*len(dataloader)))\n",
        "        print('Epoch {}, Loss {}'.format(epoch, loss_all / len(dataloader)))\n",
        "\n",
        "        if epoch % log_interval == 0:\n",
        "            model.eval()\n",
        "            emb, y = model.encoder.get_embeddings(dataloader_eval)\n",
        "            acc_val, acc = evaluate_embedding(emb, y)\n",
        "            accuracies['val'].append(acc_val)\n",
        "            accuracies['test'].append(acc)\n",
        "            print(accuracies['val'][-1], accuracies['test'][-1])\n",
        "\n",
        "    plt.plot(loss_list)\n",
        "    plt.plot(accuracies['val'])\n",
        "    plt.plot(accuracies['test'])\n",
        "\n",
        "    # tpe  = ('local' if args.local else '') + ('prior' if args.prior else '')\n",
        "    # with open('logs/log_' + args.DS + '_' + args.aug, 'a+') as f:\n",
        "    #     s = json.dumps(accuracies)\n",
        "    #     f.write('{},{},{},{},{},{},{}\\n'.format(args.DS, tpe, args.num_gc_layers, epochs, log_interval, lr, s))\n",
        "    #     f.write('\\n')\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
